{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation으로 MNIST 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape =  (60000, 785) test data shape =  (10000, 785)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "# data 불러오기\r\n",
    "train_data = np.loadtxt('C:/Users/user/Desktop/work/neowizard/MachineLearning/mnist_train.csv', delimiter=',', dtype=np.float32)\r\n",
    "test_data = np.loadtxt('C:/Users/user/Desktop/work/neowizard/MachineLearning/mnist_test.csv', delimiter=',', dtype=np.float32)\r\n",
    "\r\n",
    "print(\"train data shape = \", train_data.shape, \"test data shape = \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data에서 가장 첫 번째 이미지 출력해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAN8klEQVR4nO3df6jVdZ7H8ddrbfojxzI39iZOrWOEUdE6i9nSyjYRTj8o7FYMIzQ0JDl/JDSwyIb7xxSLIVu6rBSDDtXYMus0UJHFMNVm5S6BdDMrs21qoxjlphtmmv1a9b1/3K9xp+75nOs53/PD+34+4HDO+b7P93zffPHl99f53o8jQgAmvj/rdQMAuoOwA0kQdiAJwg4kQdiBJE7o5sJsc+of6LCI8FjT29qy277C9lu237F9ezvfBaCz3Op1dtuTJP1B0gJJOyW9JGlRROwozMOWHeiwTmzZ50l6JyLejYgvJf1G0sI2vg9AB7UT9hmS/jjq/c5q2p+wvcT2kO2hNpYFoE0dP0EXEeskrZPYjQd6qZ0t+y5JZ4x6/51qGoA+1E7YX5J0tu3v2j5R0o8kbaynLQB1a3k3PiIO2V4q6SlJkyQ9EBFv1NYZgFq1fOmtpYVxzA50XEd+VAPg+EHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEi0P2Yzjw6RJk4r1U045paPLX7p0acPaSSedVJx39uzZxfqtt95arN9zzz0Na4sWLSrO+/nnnxfrK1euLNbvvPPOYr0X2gq77fckHZB0WNKhiJhbR1MA6lfHlv3SiPiwhu8B0EEcswNJtBv2kPS07ZdtLxnrA7aX2B6yPdTmsgC0od3d+PkRscv2X0h6xvZ/R8Tm0R+IiHWS1kmS7WhzeQBa1NaWPSJ2Vc97JD0maV4dTQGoX8thtz3Z9pSjryX9QNL2uhoDUK92duMHJD1m++j3/HtE/L6WriaYM888s1g/8cQTi/WLL764WJ8/f37D2tSpU4vzXn/99cV6L+3cubNYX7NmTbE+ODjYsHbgwIHivK+++mqx/sILLxTr/ajlsEfEu5L+qsZeAHQQl96AJAg7kARhB5Ig7EAShB1IwhHd+1HbRP0F3Zw5c4r1TZs2Feudvs20Xx05cqRYv/nmm4v1Tz75pOVlDw8PF+sfffRRsf7WW2+1vOxOiwiPNZ0tO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXX2GkybNq1Y37JlS7E+a9asOtupVbPe9+3bV6xfeumlDWtffvllcd6svz9oF9fZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJhmyuwd69e4v1ZcuWFetXX311sf7KK68U683+pHLJtm3bivUFCxYU6wcPHizWzzvvvIa12267rTgv6sWWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4H72PnDyyScX682GF167dm3D2uLFi4vz3njjjcX6hg0binX0n5bvZ7f9gO09trePmjbN9jO2366eT62zWQD1G89u/K8kXfG1abdLejYizpb0bPUeQB9rGvaI2Czp678HXShpffV6vaRr620LQN1a/W38QEQcHSzrA0kDjT5oe4mkJS0uB0BN2r4RJiKidOItItZJWidxgg7opVYvve22PV2Squc99bUEoBNaDftGSTdVr2+S9Hg97QDolKa78bY3SPq+pNNs75T0c0krJf3W9mJJ70v6YSebnOj279/f1vwff/xxy/PecsstxfrDDz9crDcbYx39o2nYI2JRg9JlNfcCoIP4uSyQBGEHkiDsQBKEHUiCsANJcIvrBDB58uSGtSeeeKI47yWXXFKsX3nllcX6008/Xayj+xiyGUiOsANJEHYgCcIOJEHYgSQIO5AEYQeS4Dr7BHfWWWcV61u3bi3W9+3bV6w/99xzxfrQ0FDD2n333Vect5v/NicSrrMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJcZ09ucHCwWH/wwQeL9SlTprS87OXLlxfrDz30ULE+PDxcrGfFdXYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSILr7Cg6//zzi/XVq1cX65dd1vpgv2vXri3WV6xYUazv2rWr5WUfz1q+zm77Adt7bG8fNe0O27tsb6seV9XZLID6jWc3/leSrhhj+r9ExJzq8bt62wJQt6Zhj4jNkvZ2oRcAHdTOCbqltl+rdvNPbfQh20tsD9lu/MfIAHRcq2H/haSzJM2RNCxpVaMPRsS6iJgbEXNbXBaAGrQU9ojYHRGHI+KIpF9KmldvWwDq1lLYbU8f9XZQ0vZGnwXQH5peZ7e9QdL3JZ0mabekn1fv50gKSe9J+mlENL25mOvsE8/UqVOL9WuuuaZhrdm98vaYl4u/smnTpmJ9wYIFxfpE1eg6+wnjmHHRGJPvb7sjAF3Fz2WBJAg7kARhB5Ig7EAShB1Igltc0TNffPFFsX7CCeWLRYcOHSrWL7/88oa1559/vjjv8Yw/JQ0kR9iBJAg7kARhB5Ig7EAShB1IgrADSTS96w25XXDBBcX6DTfcUKxfeOGFDWvNrqM3s2PHjmJ98+bNbX3/RMOWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4Dr7BDd79uxifenSpcX6ddddV6yffvrpx9zTeB0+fLhYHx4u//XyI0eO1NnOcY8tO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXX240Cza9mLFo010O6IZtfRZ86c2UpLtRgaGirWV6xYUaxv3LixznYmvKZbdttn2H7O9g7bb9i+rZo+zfYztt+unk/tfLsAWjWe3fhDkv4+Is6V9DeSbrV9rqTbJT0bEWdLerZ6D6BPNQ17RAxHxNbq9QFJb0qaIWmhpPXVx9ZLurZDPQKowTEds9ueKel7krZIGoiIoz9O/kDSQIN5lkha0kaPAGow7rPxtr8t6RFJP4uI/aNrMTI65JiDNkbEuoiYGxFz2+oUQFvGFXbb39JI0H8dEY9Wk3fbnl7Vp0va05kWAdSh6W68bUu6X9KbEbF6VGmjpJskrayeH+9IhxPAwMCYRzhfOffcc4v1e++9t1g/55xzjrmnumzZsqVYv/vuuxvWHn+8/E+GW1TrNZ5j9r+V9GNJr9veVk1brpGQ/9b2YknvS/phRzoEUIumYY+I/5I05uDuki6rtx0AncLPZYEkCDuQBGEHkiDsQBKEHUiCW1zHadq0aQ1ra9euLc47Z86cYn3WrFmttFSLF198sVhftWpVsf7UU08V65999tkx94TOYMsOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0mkuc5+0UUXFevLli0r1ufNm9ewNmPGjJZ6qsunn37asLZmzZrivHfddVexfvDgwZZ6Qv9hyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSaS5zj44ONhWvR07duwo1p988sli/dChQ8V66Z7zffv2FedFHmzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJR0T5A/YZkh6SNCApJK2LiH+1fYekWyT9b/XR5RHxuybfVV4YgLZFxJijLo8n7NMlTY+IrbanSHpZ0rUaGY/9k4i4Z7xNEHag8xqFfTzjsw9LGq5eH7D9pqTe/mkWAMfsmI7Zbc+U9D1JW6pJS22/ZvsB26c2mGeJ7SHbQ+21CqAdTXfjv/qg/W1JL0haERGP2h6Q9KFGjuP/SSO7+jc3+Q5244EOa/mYXZJsf0vSk5KeiojVY9RnSnoyIs5v8j2EHeiwRmFvuhtv25Lul/Tm6KBXJ+6OGpS0vd0mAXTOeM7Gz5f0n5Jel3Skmrxc0iJJczSyG/+epJ9WJ/NK38WWHeiwtnbj60LYgc5reTcewMRA2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLbQzZ/KOn9Ue9Pq6b1o37trV/7kuitVXX29peNCl29n/0bC7eHImJuzxoo6Nfe+rUvid5a1a3e2I0HkiDsQBK9Dvu6Hi+/pF9769e+JHprVVd66+kxO4Du6fWWHUCXEHYgiZ6E3fYVtt+y/Y7t23vRQyO237P9uu1tvR6frhpDb4/t7aOmTbP9jO23q+cxx9jrUW932N5Vrbtttq/qUW9n2H7O9g7bb9i+rZre03VX6Ksr663rx+y2J0n6g6QFknZKeknSoojY0dVGGrD9nqS5EdHzH2DY/jtJn0h66OjQWrb/WdLeiFhZ/Ud5akT8Q5/0doeOcRjvDvXWaJjxn6iH667O4c9b0Yst+zxJ70TEuxHxpaTfSFrYgz76XkRslrT3a5MXSlpfvV6vkX8sXdegt74QEcMRsbV6fUDS0WHGe7ruCn11RS/CPkPSH0e936n+Gu89JD1t+2XbS3rdzBgGRg2z9YGkgV42M4amw3h309eGGe+bddfK8Oft4gTdN82PiL+WdKWkW6vd1b4UI8dg/XTt9BeSztLIGIDDklb1splqmPFHJP0sIvaPrvVy3Y3RV1fWWy/CvkvSGaPef6ea1hciYlf1vEfSYxo57Ognu4+OoFs97+lxP1+JiN0RcTgijkj6pXq47qphxh+R9OuIeLSa3PN1N1Zf3VpvvQj7S5LOtv1d2ydK+pGkjT3o4xtsT65OnMj2ZEk/UP8NRb1R0k3V65skPd7DXv5Evwzj3WiYcfV43fV8+POI6PpD0lUaOSP/P5L+sRc9NOhrlqRXq8cbve5N0gaN7Nb9n0bObSyW9OeSnpX0tqT/kDStj3r7N40M7f2aRoI1vUe9zdfILvprkrZVj6t6ve4KfXVlvfFzWSAJTtABSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBL/DyJ7caZa7LphAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = train_data[0][1:].reshape(28, 28)\r\n",
    "\r\n",
    "plt.imshow(img, cmap='gray')\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드 구현\r\n",
    "### External function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\r\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\r\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\r\n",
    "        self.input_nodes = input_nodes\r\n",
    "        self.hidden_nodes = hidden_nodes\r\n",
    "        self.output_nodes = output_nodes\r\n",
    "\r\n",
    "        # 은닉층 가중치 W2 = (784 * 100) Xavier/He 방법으로 초기화\r\n",
    "        self.W2 = np.random.randn(self.input_nodes, self.hidden_nodes) / np.sqrt(self.input_nodes/2)\r\n",
    "        self.b2 = np.random.rand(self.hidden_nodes)\r\n",
    "\r\n",
    "        # 출력층 가중치 W3 = (100 * 10) Xavier/He 방법으로 초기화\r\n",
    "        self.W3 = np.random.randn(hidden_nodes, output_nodes) / np.sqrt(self.hidden_nodes/2)\r\n",
    "        self.b3 = np.random.rand(output_nodes)\r\n",
    "\r\n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 정의\r\n",
    "        self.Z3 = np.zeros([1, output_nodes])\r\n",
    "        self.A3 = np.zeros([1, output_nodes])\r\n",
    "\r\n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 정의\r\n",
    "        self.Z2 = np.zeros([1, hidden_nodes])\r\n",
    "        self.A2 = np.zeros([1, hidden_nodes])\r\n",
    "\r\n",
    "        # 입력층 선형회쉬 값 Z1, 출력값 A1 정의\r\n",
    "        self.Z1 = np.zeros([1, input_nodes])\r\n",
    "        self.A1 = np.zeros([1, input_nodes])\r\n",
    "\r\n",
    "        # 학습률 초기화\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "\r\n",
    "        print(\"Neural Network object is created!\")\r\n",
    "    \r\n",
    "    def feed_forward(self):\r\n",
    "        delta = 1e-7\r\n",
    "\r\n",
    "        # 입력층 선형회귀 값 Z1, 출력값 A1 계산\r\n",
    "        self.Z1 = self.input_data\r\n",
    "        self.A1 = self.input_data\r\n",
    "\r\n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산\r\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\r\n",
    "        self.A2 = sigmoid(self.Z2)\r\n",
    "\r\n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\r\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\r\n",
    "        self.A3 = sigmoid(self.Z3)\r\n",
    "\r\n",
    "        return -np.sum( self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1-self.A3)+delta) )\r\n",
    "\r\n",
    "    def loss_val(self):\r\n",
    "        delta = 1e-7\r\n",
    "\r\n",
    "        self.Z1 = self.input_data\r\n",
    "        self.A1 = self.input_data\r\n",
    "\r\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\r\n",
    "        self.A2 = sigmoid(self.Z2)\r\n",
    "\r\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\r\n",
    "        self.A3 = sigmoid(self.Z3)\r\n",
    "\r\n",
    "        return -np.sum( self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1-self.A3)+delta) )\r\n",
    "\r\n",
    "    def train(self, input_data, target_data):\r\n",
    "        self.input_data = input_data\r\n",
    "        self.target_data = target_data\r\n",
    "\r\n",
    "        # 먼저 feed forward를 통해서 최종 출력값과 이를 바탕으로 현재의 에러 값 계산\r\n",
    "        loss_val = self.feed_forward()\r\n",
    "\r\n",
    "        # 출력층 loss인 loss_3 구하기\r\n",
    "        loss_3 = (self.A3 - self.target_data)*(self.A3*(1-self.A3))\r\n",
    "\r\n",
    "        # 출력층 가중치 W3, 출력층 바이어스 b3 업데이트\r\n",
    "        self.W3 = self.W3 - self.learning_rate*np.dot(self.A2.T, loss_3)\r\n",
    "        self.b3 = self.b3 - self.learning_rate*loss_3\r\n",
    "\r\n",
    "        # 은닉층 loss인 loss_2 구하기\r\n",
    "        loss_2 = np.dot(loss_3, self.W3.T) * self.A2 * (1-self.A2)\r\n",
    "\r\n",
    "        # 은닉층 가중치 W2, 은닉층 바이어스 b2 업데이트\r\n",
    "        self.W2 = self.W2 - self.learning_rate*np.dot(self.A1.T, loss_2)\r\n",
    "        self.b2 = self.b2 - self.learning_rate*loss_2\r\n",
    "\r\n",
    "    def predict(self, input_data):\r\n",
    "        Z2 = np.dot(input_data, self.W2) + self.b2\r\n",
    "        A2 = sigmoid(Z2)\r\n",
    "\r\n",
    "        Z3 = np.dot(A2, self.W3) + self.b3\r\n",
    "        A3 = sigmoid(Z3)\r\n",
    "\r\n",
    "        predicted_num = np.argmax(A3)\r\n",
    "\r\n",
    "        return predicted_num\r\n",
    "    \r\n",
    "    # 정확도 측정 함수\r\n",
    "    def accuracy(self, test_data):\r\n",
    "        matched_list = []\r\n",
    "        not_matched_list = []\r\n",
    "\r\n",
    "        for index in range(len(test_data)):\r\n",
    "            label = int(test_data[index, 0])  # test_data의 1열에 있는 정답 분리\r\n",
    "\r\n",
    "            # one-hot encoding을 위한 데이터 정규화 (data normalize)\r\n",
    "            data = (test_data[index, 1:] / 255.0 * 0.99) + 0.01\r\n",
    "\r\n",
    "            # predict를 위해서 vector를 matrix로 변환하여 인수로 넘겨준다\r\n",
    "            predicted_num = self.predict(np.array(data, ndmin=2))\r\n",
    "\r\n",
    "            if label == predicted_num:\r\n",
    "                matched_list.append(index)\r\n",
    "            else:\r\n",
    "                not_matched_list.append(index)\r\n",
    "            \r\n",
    "        print(\"Current Accuracy = \", 100*(len(matched_list)/(len(test_data))), \" %\")\r\n",
    "\r\n",
    "        return matched_list, not_matched_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network object is created!\n",
      "step =  0 ,  loss_val =  4.331931817992122\n",
      "step =  400 ,  loss_val =  1.694810865293667\n",
      "step =  800 ,  loss_val =  1.497335759690883\n",
      "step =  1200 ,  loss_val =  0.7525061463059861\n",
      "step =  1600 ,  loss_val =  0.9863757851813293\n",
      "step =  2000 ,  loss_val =  1.110104975027212\n",
      "step =  2400 ,  loss_val =  0.6951055240017249\n",
      "step =  2800 ,  loss_val =  0.823179058325709\n",
      "step =  3200 ,  loss_val =  0.750441818455847\n",
      "step =  3600 ,  loss_val =  0.6988741203011237\n",
      "step =  4000 ,  loss_val =  0.9020161382400932\n",
      "step =  4400 ,  loss_val =  0.795068651953615\n",
      "step =  4800 ,  loss_val =  0.7288046804568401\n",
      "step =  5200 ,  loss_val =  1.1406339410708322\n",
      "step =  5600 ,  loss_val =  1.0666401040068219\n",
      "step =  6000 ,  loss_val =  0.8131935007500805\n",
      "step =  6400 ,  loss_val =  0.8543070115906501\n",
      "step =  6800 ,  loss_val =  0.9373144627016758\n",
      "step =  7200 ,  loss_val =  0.8153872251029965\n",
      "step =  7600 ,  loss_val =  0.917955893580041\n",
      "step =  8000 ,  loss_val =  0.9732588389604825\n",
      "step =  8400 ,  loss_val =  0.7883704971993679\n",
      "step =  8800 ,  loss_val =  0.8788483408164707\n",
      "step =  9200 ,  loss_val =  0.8647698066479453\n",
      "step =  9600 ,  loss_val =  0.9012880846225599\n",
      "step =  10000 ,  loss_val =  0.9847443349845485\n",
      "step =  10400 ,  loss_val =  0.8419152018385758\n",
      "step =  10800 ,  loss_val =  5.909011367621963\n",
      "step =  11200 ,  loss_val =  0.9184850398556454\n",
      "step =  11600 ,  loss_val =  11.322103774464614\n",
      "step =  12000 ,  loss_val =  0.9202722242603267\n",
      "step =  12400 ,  loss_val =  0.8858614666165523\n",
      "step =  12800 ,  loss_val =  0.8707633570374237\n",
      "step =  13200 ,  loss_val =  1.0261531981864205\n",
      "step =  13600 ,  loss_val =  0.9961464955844561\n",
      "step =  14000 ,  loss_val =  0.9355371242511061\n",
      "step =  14400 ,  loss_val =  1.02687282566947\n",
      "step =  14800 ,  loss_val =  0.9883318861750723\n",
      "step =  15200 ,  loss_val =  1.0570324873663983\n",
      "step =  15600 ,  loss_val =  1.0219720327453103\n",
      "step =  16000 ,  loss_val =  0.9726855489427632\n",
      "step =  16400 ,  loss_val =  0.9052840700831895\n",
      "step =  16800 ,  loss_val =  1.0009048312518047\n",
      "step =  17200 ,  loss_val =  0.9306582543333427\n",
      "step =  17600 ,  loss_val =  1.0250465203851649\n",
      "step =  18000 ,  loss_val =  0.9983478432991663\n",
      "step =  18400 ,  loss_val =  0.9137588265627504\n",
      "step =  18800 ,  loss_val =  0.9770930759320314\n",
      "step =  19200 ,  loss_val =  0.9783521005773201\n",
      "step =  19600 ,  loss_val =  0.9478977066911961\n",
      "step =  20000 ,  loss_val =  1.046268611798044\n",
      "step =  20400 ,  loss_val =  0.9738602729169049\n",
      "step =  20800 ,  loss_val =  1.063838440017586\n",
      "step =  21200 ,  loss_val =  0.9041417493662464\n",
      "step =  21600 ,  loss_val =  0.9832479777401468\n",
      "step =  22000 ,  loss_val =  0.9717051070871725\n",
      "step =  22400 ,  loss_val =  0.9417595519242314\n",
      "step =  22800 ,  loss_val =  1.0023828584146168\n",
      "step =  23200 ,  loss_val =  0.9351492406918457\n",
      "step =  23600 ,  loss_val =  1.065872775609267\n",
      "step =  24000 ,  loss_val =  1.0438349306540586\n",
      "step =  24400 ,  loss_val =  0.9840134939105976\n",
      "step =  24800 ,  loss_val =  1.0135463803427025\n",
      "step =  25200 ,  loss_val =  1.0694637768868598\n",
      "step =  25600 ,  loss_val =  0.9682619916676838\n",
      "step =  26000 ,  loss_val =  0.9756738464294258\n",
      "step =  26400 ,  loss_val =  1.0013210747413368\n",
      "step =  26800 ,  loss_val =  1.2462393549954567\n",
      "step =  27200 ,  loss_val =  1.0919027111022521\n",
      "step =  27600 ,  loss_val =  1.1329940636311906\n",
      "step =  28000 ,  loss_val =  0.9611746896267587\n",
      "step =  28400 ,  loss_val =  1.139111767708669\n",
      "step =  28800 ,  loss_val =  0.9916219345018039\n",
      "step =  29200 ,  loss_val =  1.0494363450682251\n",
      "step =  29600 ,  loss_val =  0.9977348070468458\n",
      "step =  30000 ,  loss_val =  1.1117750235435135\n",
      "step =  30400 ,  loss_val =  0.9141929285935196\n",
      "step =  30800 ,  loss_val =  1.063464696470585\n",
      "step =  31200 ,  loss_val =  8.762710392049282\n",
      "step =  31600 ,  loss_val =  2.9776051316715706\n",
      "step =  32000 ,  loss_val =  0.9717454734418896\n",
      "step =  32400 ,  loss_val =  1.0108006423948146\n",
      "step =  32800 ,  loss_val =  1.0184147458314161\n",
      "step =  33200 ,  loss_val =  1.0489594528854882\n",
      "step =  33600 ,  loss_val =  1.0220048108144606\n",
      "step =  34000 ,  loss_val =  1.13544387807124\n",
      "step =  34400 ,  loss_val =  1.0750701618599139\n",
      "step =  34800 ,  loss_val =  6.414146210161723\n",
      "step =  35200 ,  loss_val =  1.116322625076548\n",
      "step =  35600 ,  loss_val =  1.0043762727583982\n",
      "step =  36000 ,  loss_val =  0.9835724637607449\n",
      "step =  36400 ,  loss_val =  1.0885043735495123\n",
      "step =  36800 ,  loss_val =  1.049768513263804\n",
      "step =  37200 ,  loss_val =  1.041552088171601\n",
      "step =  37600 ,  loss_val =  1.0492970455697963\n",
      "step =  38000 ,  loss_val =  1.0110667056931062\n",
      "step =  38400 ,  loss_val =  1.163987433223052\n",
      "step =  38800 ,  loss_val =  1.1191044363699798\n",
      "step =  39200 ,  loss_val =  1.1040458855963962\n",
      "step =  39600 ,  loss_val =  2.832451953608899\n",
      "step =  40000 ,  loss_val =  1.1896250690413472\n",
      "step =  40400 ,  loss_val =  1.1387591044377579\n",
      "step =  40800 ,  loss_val =  1.0009340169430287\n",
      "step =  41200 ,  loss_val =  1.0390621491672376\n",
      "step =  41600 ,  loss_val =  1.0151560663526824\n",
      "step =  42000 ,  loss_val =  1.0055792124601213\n",
      "step =  42400 ,  loss_val =  1.0785701293651269\n",
      "step =  42800 ,  loss_val =  1.1368736411584026\n",
      "step =  43200 ,  loss_val =  1.1367645979146397\n",
      "step =  43600 ,  loss_val =  1.0427368562504917\n",
      "step =  44000 ,  loss_val =  1.1681210842947132\n",
      "step =  44400 ,  loss_val =  1.0850085806111673\n",
      "step =  44800 ,  loss_val =  1.02284039767101\n",
      "step =  45200 ,  loss_val =  1.2373632265314602\n",
      "step =  45600 ,  loss_val =  1.0623515563607289\n",
      "step =  46000 ,  loss_val =  1.004589288612158\n",
      "step =  46400 ,  loss_val =  1.1537248574450027\n",
      "step =  46800 ,  loss_val =  1.0917241233658506\n",
      "step =  47200 ,  loss_val =  1.0761386783524907\n",
      "step =  47600 ,  loss_val =  9.570573394896375\n",
      "step =  48000 ,  loss_val =  1.0901128812486878\n",
      "step =  48400 ,  loss_val =  1.1721719997316735\n",
      "step =  48800 ,  loss_val =  1.0136293300037078\n",
      "step =  49200 ,  loss_val =  1.3609242670921202\n",
      "step =  49600 ,  loss_val =  1.0243786180515158\n",
      "step =  50000 ,  loss_val =  1.1418484389351193\n",
      "step =  50400 ,  loss_val =  0.9897085399742338\n",
      "step =  50800 ,  loss_val =  1.125576639595541\n",
      "step =  51200 ,  loss_val =  0.9851268415827527\n",
      "step =  51600 ,  loss_val =  16.321622155692882\n",
      "step =  52000 ,  loss_val =  1.080531596209951\n",
      "step =  52400 ,  loss_val =  1.2043262775510122\n",
      "step =  52800 ,  loss_val =  3.2218495846667263\n",
      "step =  53200 ,  loss_val =  1.0356400453172403\n",
      "step =  53600 ,  loss_val =  1.0830298423966778\n",
      "step =  54000 ,  loss_val =  1.0630999359174131\n",
      "step =  54400 ,  loss_val =  1.0866412229224018\n",
      "step =  54800 ,  loss_val =  1.1324613000012167\n",
      "step =  55200 ,  loss_val =  1.1253228100752266\n",
      "step =  55600 ,  loss_val =  1.0404629942196089\n",
      "step =  56000 ,  loss_val =  1.0438556489585553\n",
      "step =  56400 ,  loss_val =  1.0821601882315368\n",
      "step =  56800 ,  loss_val =  10.683495984667449\n",
      "step =  57200 ,  loss_val =  1.070522414918406\n",
      "step =  57600 ,  loss_val =  1.2117555929040167\n",
      "step =  58000 ,  loss_val =  1.1122928679451238\n",
      "step =  58400 ,  loss_val =  1.011852480612323\n",
      "step =  58800 ,  loss_val =  1.1847763352162664\n",
      "step =  59200 ,  loss_val =  1.05066363213361\n",
      "step =  59600 ,  loss_val =  1.1024245696788455\n",
      "step =  0 ,  loss_val =  1.1882669962445591\n",
      "step =  400 ,  loss_val =  1.2160937139297283\n",
      "step =  800 ,  loss_val =  1.0788159117069571\n",
      "step =  1200 ,  loss_val =  1.0545247290050934\n",
      "step =  1600 ,  loss_val =  1.2147519469705825\n",
      "step =  2000 ,  loss_val =  1.485259488924786\n",
      "step =  2400 ,  loss_val =  1.0680972130506825\n",
      "step =  2800 ,  loss_val =  1.1029487991214255\n",
      "step =  3200 ,  loss_val =  1.185956518561005\n",
      "step =  3600 ,  loss_val =  1.060972812157718\n",
      "step =  4000 ,  loss_val =  1.2409269557666858\n",
      "step =  4400 ,  loss_val =  1.1142003831762175\n",
      "step =  4800 ,  loss_val =  1.0576999581671267\n",
      "step =  5200 ,  loss_val =  1.0240419719819334\n",
      "step =  5600 ,  loss_val =  1.070148706994855\n",
      "step =  6000 ,  loss_val =  1.1257169591293805\n",
      "step =  6400 ,  loss_val =  1.1321427703640052\n",
      "step =  6800 ,  loss_val =  1.0095080531435756\n",
      "step =  7200 ,  loss_val =  1.0618125538464662\n",
      "step =  7600 ,  loss_val =  1.2054125413463834\n",
      "step =  8000 ,  loss_val =  1.2771659514800138\n",
      "step =  8400 ,  loss_val =  1.0107586784955076\n",
      "step =  8800 ,  loss_val =  1.22091387590976\n",
      "step =  9200 ,  loss_val =  1.2383108975887132\n",
      "step =  9600 ,  loss_val =  1.329599620068835\n",
      "step =  10000 ,  loss_val =  1.2751403378036743\n",
      "step =  10400 ,  loss_val =  1.052604737079598\n",
      "step =  10800 ,  loss_val =  0.9935503198609817\n",
      "step =  11200 ,  loss_val =  1.0362997218904781\n",
      "step =  11600 ,  loss_val =  1.043203901284115\n",
      "step =  12000 ,  loss_val =  1.123491997425943\n",
      "step =  12400 ,  loss_val =  1.0819264192794207\n",
      "step =  12800 ,  loss_val =  1.1026244414139996\n",
      "step =  13200 ,  loss_val =  1.094369656212374\n",
      "step =  13600 ,  loss_val =  1.2134733341117314\n",
      "step =  14000 ,  loss_val =  1.0800750256014717\n",
      "step =  14400 ,  loss_val =  1.1667768707289952\n",
      "step =  14800 ,  loss_val =  1.1769640846509168\n",
      "step =  15200 ,  loss_val =  1.2861509625326268\n",
      "step =  15600 ,  loss_val =  1.2055365673613496\n",
      "step =  16000 ,  loss_val =  1.1595698101513374\n",
      "step =  16400 ,  loss_val =  1.147352694110496\n",
      "step =  16800 ,  loss_val =  1.282502598851396\n",
      "step =  17200 ,  loss_val =  1.1481235127567997\n",
      "step =  17600 ,  loss_val =  1.145461334862938\n",
      "step =  18000 ,  loss_val =  1.091248687666611\n",
      "step =  18400 ,  loss_val =  1.046725326694862\n",
      "step =  18800 ,  loss_val =  1.0852161320037155\n",
      "step =  19200 ,  loss_val =  1.2060063201960118\n",
      "step =  19600 ,  loss_val =  1.1010455177271128\n",
      "step =  20000 ,  loss_val =  1.203799308326142\n",
      "step =  20400 ,  loss_val =  1.2860766945860098\n",
      "step =  20800 ,  loss_val =  1.273070718680823\n",
      "step =  21200 ,  loss_val =  1.048158407553709\n",
      "step =  21600 ,  loss_val =  1.1170655682698643\n",
      "step =  22000 ,  loss_val =  1.1855898167760812\n",
      "step =  22400 ,  loss_val =  1.035341640574582\n",
      "step =  22800 ,  loss_val =  1.072594241620791\n",
      "step =  23200 ,  loss_val =  1.0883009859521553\n",
      "step =  23600 ,  loss_val =  1.0315848779482506\n",
      "step =  24000 ,  loss_val =  1.1463291619322966\n",
      "step =  24400 ,  loss_val =  1.236001215583125\n",
      "step =  24800 ,  loss_val =  1.0237438843584044\n",
      "step =  25200 ,  loss_val =  1.0786872970414556\n",
      "step =  25600 ,  loss_val =  1.1155328624552432\n",
      "step =  26000 ,  loss_val =  1.1689708289316192\n",
      "step =  26400 ,  loss_val =  1.159202626851097\n",
      "step =  26800 ,  loss_val =  1.3934969679215792\n",
      "step =  27200 ,  loss_val =  1.044630809478249\n",
      "step =  27600 ,  loss_val =  1.3219489231660804\n",
      "step =  28000 ,  loss_val =  1.0767190217719078\n",
      "step =  28400 ,  loss_val =  1.241134586828184\n",
      "step =  28800 ,  loss_val =  1.0455236628581797\n",
      "step =  29200 ,  loss_val =  1.1664330688734692\n",
      "step =  29600 ,  loss_val =  1.1337257750474588\n",
      "step =  30000 ,  loss_val =  1.2147961729353502\n",
      "step =  30400 ,  loss_val =  1.0228553654449803\n",
      "step =  30800 ,  loss_val =  1.0834443140150456\n",
      "step =  31200 ,  loss_val =  1.1980883847843888\n",
      "step =  31600 ,  loss_val =  5.849105488714649\n",
      "step =  32000 ,  loss_val =  1.0810043352686811\n",
      "step =  32400 ,  loss_val =  1.0671704286610109\n",
      "step =  32800 ,  loss_val =  1.1848723415678322\n",
      "step =  33200 ,  loss_val =  1.0581849075518903\n",
      "step =  33600 ,  loss_val =  1.0908546129661718\n",
      "step =  34000 ,  loss_val =  1.1344395928642983\n",
      "step =  34400 ,  loss_val =  1.1707003471657256\n",
      "step =  34800 ,  loss_val =  2.2352058277799833\n",
      "step =  35200 ,  loss_val =  1.1268977402138718\n",
      "step =  35600 ,  loss_val =  1.117473329815118\n",
      "step =  36000 ,  loss_val =  0.9983208712065441\n",
      "step =  36400 ,  loss_val =  1.161055353531494\n",
      "step =  36800 ,  loss_val =  1.0844733005674898\n",
      "step =  37200 ,  loss_val =  1.0893683158859622\n",
      "step =  37600 ,  loss_val =  1.0242118841862\n",
      "step =  38000 ,  loss_val =  1.0642302753882187\n",
      "step =  38400 ,  loss_val =  1.2009537833181607\n",
      "step =  38800 ,  loss_val =  1.2181455475216671\n",
      "step =  39200 ,  loss_val =  1.26634529322273\n",
      "step =  39600 ,  loss_val =  1.0863770709445153\n",
      "step =  40000 ,  loss_val =  1.2773731722103352\n",
      "step =  40400 ,  loss_val =  1.1816570332677496\n",
      "step =  40800 ,  loss_val =  1.1137111856222364\n",
      "step =  41200 ,  loss_val =  1.0465177173510867\n",
      "step =  41600 ,  loss_val =  1.120601424690969\n",
      "step =  42000 ,  loss_val =  1.168120922128512\n",
      "step =  42400 ,  loss_val =  1.2181210860521352\n",
      "step =  42800 ,  loss_val =  1.2011957768794779\n",
      "step =  43200 ,  loss_val =  1.1625305563334534\n",
      "step =  43600 ,  loss_val =  1.1297260628185382\n",
      "step =  44000 ,  loss_val =  1.215390696566434\n",
      "step =  44400 ,  loss_val =  1.1402700520275164\n",
      "step =  44800 ,  loss_val =  1.0797280347656057\n",
      "step =  45200 ,  loss_val =  1.2890060896057378\n",
      "step =  45600 ,  loss_val =  1.1538653326415422\n",
      "step =  46000 ,  loss_val =  1.1227228433447882\n",
      "step =  46400 ,  loss_val =  1.1355171258339622\n",
      "step =  46800 ,  loss_val =  1.1909356675835039\n",
      "step =  47200 ,  loss_val =  1.15602248347191\n",
      "step =  47600 ,  loss_val =  13.516564861227085\n",
      "step =  48000 ,  loss_val =  1.1388823089571423\n",
      "step =  48400 ,  loss_val =  1.2698875920605397\n",
      "step =  48800 ,  loss_val =  1.1179576073996176\n",
      "step =  49200 ,  loss_val =  1.1315692890511675\n",
      "step =  49600 ,  loss_val =  0.9814262182206864\n",
      "step =  50000 ,  loss_val =  1.2133386997927682\n",
      "step =  50400 ,  loss_val =  1.046740171646289\n",
      "step =  50800 ,  loss_val =  1.1701937692773703\n",
      "step =  51200 ,  loss_val =  3.172069719461175\n",
      "step =  51600 ,  loss_val =  15.117128006641092\n",
      "step =  52000 ,  loss_val =  1.0810298020526485\n",
      "step =  52400 ,  loss_val =  1.2148865629034298\n",
      "step =  52800 ,  loss_val =  7.739773756725716\n",
      "step =  53200 ,  loss_val =  1.0786589307108865\n",
      "step =  53600 ,  loss_val =  1.2005375783016212\n",
      "step =  54000 ,  loss_val =  1.1768495843775564\n",
      "step =  54400 ,  loss_val =  1.1744009715347077\n",
      "step =  54800 ,  loss_val =  1.2104564862081313\n",
      "step =  55200 ,  loss_val =  1.099170199254092\n",
      "step =  55600 ,  loss_val =  1.1263202763680835\n",
      "step =  56000 ,  loss_val =  1.0817382532205992\n",
      "step =  56400 ,  loss_val =  1.1687385130549302\n",
      "step =  56800 ,  loss_val =  11.965257812543756\n",
      "step =  57200 ,  loss_val =  1.1022744634048736\n",
      "step =  57600 ,  loss_val =  1.25476331747771\n",
      "step =  58000 ,  loss_val =  1.1908600232098152\n",
      "step =  58400 ,  loss_val =  1.0910032347748748\n",
      "step =  58800 ,  loss_val =  1.2338655025541427\n",
      "step =  59200 ,  loss_val =  1.1257422659662004\n",
      "step =  59600 ,  loss_val =  1.1411527489393773\n",
      "step =  0 ,  loss_val =  1.1880725249956676\n",
      "step =  400 ,  loss_val =  1.391453665340703\n",
      "step =  800 ,  loss_val =  1.1380206174513887\n",
      "step =  1200 ,  loss_val =  1.20107825392969\n",
      "step =  1600 ,  loss_val =  1.1477397232789757\n",
      "step =  2000 ,  loss_val =  1.0918984540803156\n",
      "step =  2400 ,  loss_val =  1.1722379232668838\n",
      "step =  2800 ,  loss_val =  1.1231590789700692\n",
      "step =  3200 ,  loss_val =  1.2539474131201769\n",
      "step =  3600 ,  loss_val =  1.1239529630276253\n",
      "step =  4000 ,  loss_val =  1.265139942856893\n",
      "step =  4400 ,  loss_val =  1.2244997714001278\n",
      "step =  4800 ,  loss_val =  1.0770972884848502\n",
      "step =  5200 ,  loss_val =  1.1429539061927507\n",
      "step =  5600 ,  loss_val =  1.1841617265299011\n",
      "step =  6000 ,  loss_val =  1.2083602512719542\n",
      "step =  6400 ,  loss_val =  1.1457799180586825\n",
      "step =  6800 ,  loss_val =  1.060096661401168\n",
      "step =  7200 ,  loss_val =  1.078514535892965\n",
      "step =  7600 ,  loss_val =  1.2932538992635794\n",
      "step =  8000 ,  loss_val =  1.2879436798969932\n",
      "step =  8400 ,  loss_val =  1.1128513219083247\n",
      "step =  8800 ,  loss_val =  1.2343797789692543\n",
      "step =  9200 ,  loss_val =  1.2187222022522195\n",
      "step =  9600 ,  loss_val =  1.3571850657221227\n",
      "step =  10000 ,  loss_val =  1.256740137937826\n",
      "step =  10400 ,  loss_val =  1.1925106482071803\n",
      "step =  10800 ,  loss_val =  1.0563176418455626\n",
      "step =  11200 ,  loss_val =  1.0971467291936927\n",
      "step =  11600 ,  loss_val =  1.1492669086719847\n",
      "step =  12000 ,  loss_val =  1.16124043088768\n",
      "step =  12400 ,  loss_val =  1.0958026772737532\n",
      "step =  12800 ,  loss_val =  1.1280898396373529\n",
      "step =  13200 ,  loss_val =  1.187456397810525\n",
      "step =  13600 ,  loss_val =  1.2854282593287605\n",
      "step =  14000 ,  loss_val =  1.1341809456237124\n",
      "step =  14400 ,  loss_val =  1.1562834479458237\n",
      "step =  14800 ,  loss_val =  1.1446665069786435\n",
      "step =  15200 ,  loss_val =  1.3978926782303467\n",
      "step =  15600 ,  loss_val =  1.189414856876123\n",
      "step =  16000 ,  loss_val =  1.2234469983062795\n",
      "step =  16400 ,  loss_val =  1.2500348382042188\n",
      "step =  16800 ,  loss_val =  1.3022094937995938\n",
      "step =  17200 ,  loss_val =  1.198530688474666\n",
      "step =  17600 ,  loss_val =  1.278723137511359\n",
      "step =  18000 ,  loss_val =  1.182521812893921\n",
      "step =  18400 ,  loss_val =  1.0737935397132012\n",
      "step =  18800 ,  loss_val =  1.091229119182059\n",
      "step =  19200 ,  loss_val =  1.2461378388948587\n",
      "step =  19600 ,  loss_val =  1.1683794676809685\n",
      "step =  20000 ,  loss_val =  1.2709402845986824\n",
      "step =  20400 ,  loss_val =  1.3545860481466823\n",
      "step =  20800 ,  loss_val =  1.3294358309252399\n",
      "step =  21200 ,  loss_val =  1.1402812730934793\n",
      "step =  21600 ,  loss_val =  1.1688249311173176\n",
      "step =  22000 ,  loss_val =  1.2681523789374407\n",
      "step =  22400 ,  loss_val =  1.1450601839568628\n",
      "step =  22800 ,  loss_val =  1.2034306521018254\n",
      "step =  23200 ,  loss_val =  1.1005083592762213\n",
      "step =  23600 ,  loss_val =  1.124303992257107\n",
      "step =  24000 ,  loss_val =  1.121570637376538\n",
      "step =  24400 ,  loss_val =  1.3003577486991071\n",
      "step =  24800 ,  loss_val =  1.0587401463270263\n",
      "step =  25200 ,  loss_val =  1.1488312172052477\n",
      "step =  25600 ,  loss_val =  1.1803314415681865\n",
      "step =  26000 ,  loss_val =  1.2823592953927205\n",
      "step =  26400 ,  loss_val =  1.172212142671932\n",
      "step =  26800 ,  loss_val =  1.4878455516695048\n",
      "step =  27200 ,  loss_val =  1.0626198004261531\n",
      "step =  27600 ,  loss_val =  1.3166982095179713\n",
      "step =  28000 ,  loss_val =  1.0936653874839815\n",
      "step =  28400 ,  loss_val =  1.252426909764142\n",
      "step =  28800 ,  loss_val =  1.1652951409335495\n",
      "step =  29200 ,  loss_val =  1.1275273400266617\n",
      "step =  29600 ,  loss_val =  1.2031114106656196\n",
      "step =  30000 ,  loss_val =  1.293135223887026\n",
      "step =  30400 ,  loss_val =  1.0650947671587176\n",
      "step =  30800 ,  loss_val =  1.1912646611705713\n",
      "step =  31200 ,  loss_val =  1.2432656589139979\n",
      "step =  31600 ,  loss_val =  1.115677514685784\n",
      "step =  32000 ,  loss_val =  1.123832127279682\n",
      "step =  32400 ,  loss_val =  1.0840122298837842\n",
      "step =  32800 ,  loss_val =  1.1627602018690781\n",
      "step =  33200 ,  loss_val =  1.0745843242111768\n",
      "step =  33600 ,  loss_val =  1.137261520749553\n",
      "step =  34000 ,  loss_val =  1.2039502460514175\n",
      "step =  34400 ,  loss_val =  1.3307324453055982\n",
      "step =  34800 ,  loss_val =  1.0695104643713504\n",
      "step =  35200 ,  loss_val =  1.1669766371489547\n",
      "step =  35600 ,  loss_val =  1.1581183360531244\n",
      "step =  36000 ,  loss_val =  1.1107664032749254\n",
      "step =  36400 ,  loss_val =  1.2009783129722134\n",
      "step =  36800 ,  loss_val =  1.0867007268760125\n",
      "step =  37200 ,  loss_val =  1.0962408317700347\n",
      "step =  37600 ,  loss_val =  1.1255645908379348\n",
      "step =  38000 ,  loss_val =  1.2008750848896073\n",
      "step =  38400 ,  loss_val =  1.2096906584243807\n",
      "step =  38800 ,  loss_val =  1.276446573642907\n",
      "step =  39200 ,  loss_val =  1.2897057218250616\n",
      "step =  39600 ,  loss_val =  0.987764005786863\n",
      "step =  40000 ,  loss_val =  1.3256521848616432\n",
      "step =  40400 ,  loss_val =  1.260236471390165\n",
      "step =  40800 ,  loss_val =  1.1137040667585016\n",
      "step =  41200 ,  loss_val =  1.1677389029452199\n",
      "step =  41600 ,  loss_val =  1.1481339971650018\n",
      "step =  42000 ,  loss_val =  1.2101201811711708\n",
      "step =  42400 ,  loss_val =  1.1867816837733256\n",
      "step =  42800 ,  loss_val =  1.2373082046176656\n",
      "step =  43200 ,  loss_val =  1.15990275817569\n",
      "step =  43600 ,  loss_val =  1.1494485974868547\n",
      "step =  44000 ,  loss_val =  1.1927357410294905\n",
      "step =  44400 ,  loss_val =  1.1868470191957265\n",
      "step =  44800 ,  loss_val =  1.1287100091456965\n",
      "step =  45200 ,  loss_val =  1.4611940929212508\n",
      "step =  45600 ,  loss_val =  1.1717801807102226\n",
      "step =  46000 ,  loss_val =  1.1714840918601452\n",
      "step =  46400 ,  loss_val =  1.1890627248298984\n",
      "step =  46800 ,  loss_val =  1.2244321968354457\n",
      "step =  47200 ,  loss_val =  1.2095915486491602\n",
      "step =  47600 ,  loss_val =  14.575953927631463\n",
      "step =  48000 ,  loss_val =  1.1671017554139977\n",
      "step =  48400 ,  loss_val =  1.2018867655266483\n",
      "step =  48800 ,  loss_val =  1.1194633904135216\n",
      "step =  49200 ,  loss_val =  1.3170025440295976\n",
      "step =  49600 ,  loss_val =  1.0948510008226175\n",
      "step =  50000 ,  loss_val =  1.2881391166740965\n",
      "step =  50400 ,  loss_val =  1.1046256892000046\n",
      "step =  50800 ,  loss_val =  1.2309226241279287\n",
      "step =  51200 ,  loss_val =  1.157493176889923\n",
      "step =  51600 ,  loss_val =  4.620580043505324\n",
      "step =  52000 ,  loss_val =  1.064407812582722\n",
      "step =  52400 ,  loss_val =  1.2801113550987842\n",
      "step =  52800 ,  loss_val =  11.210418977990008\n",
      "step =  53200 ,  loss_val =  1.1876130180977713\n",
      "step =  53600 ,  loss_val =  1.226110378166902\n",
      "step =  54000 ,  loss_val =  1.226364076863381\n",
      "step =  54400 ,  loss_val =  1.2173821883406655\n",
      "step =  54800 ,  loss_val =  1.2161514341655426\n",
      "step =  55200 ,  loss_val =  1.1270748367370191\n",
      "step =  55600 ,  loss_val =  1.1982760515311872\n",
      "step =  56000 ,  loss_val =  1.1462039334039282\n",
      "step =  56400 ,  loss_val =  1.2344100335674613\n",
      "step =  56800 ,  loss_val =  13.694005997013074\n",
      "step =  57200 ,  loss_val =  1.1642105388055548\n",
      "step =  57600 ,  loss_val =  1.2863875288500701\n",
      "step =  58000 ,  loss_val =  1.2066309429249202\n",
      "step =  58400 ,  loss_val =  1.1860958223406497\n",
      "step =  58800 ,  loss_val =  1.223118572039946\n",
      "step =  59200 ,  loss_val =  1.1852707295183822\n",
      "step =  59600 ,  loss_val =  1.1551483823600917\n",
      "step =  0 ,  loss_val =  1.3246223513839668\n",
      "step =  400 ,  loss_val =  1.3892246545884326\n",
      "step =  800 ,  loss_val =  1.1604629777412465\n",
      "step =  1200 ,  loss_val =  1.243988152187701\n",
      "step =  1600 ,  loss_val =  1.2497137109432088\n",
      "step =  2000 ,  loss_val =  1.1366106100453928\n",
      "step =  2400 ,  loss_val =  1.190563839516272\n",
      "step =  2800 ,  loss_val =  1.1826178025668557\n",
      "step =  3200 ,  loss_val =  1.2255043776884562\n",
      "step =  3600 ,  loss_val =  1.1417821561582204\n",
      "step =  4000 ,  loss_val =  1.339992921078228\n",
      "step =  4400 ,  loss_val =  1.3143326656302987\n",
      "step =  4800 ,  loss_val =  1.1595203963218266\n",
      "step =  5200 ,  loss_val =  1.1792199171266158\n",
      "step =  5600 ,  loss_val =  1.1440908544298736\n",
      "step =  6000 ,  loss_val =  1.2376325360219935\n",
      "step =  6400 ,  loss_val =  1.2398548479696336\n",
      "step =  6800 ,  loss_val =  1.1649543662376969\n",
      "step =  7200 ,  loss_val =  1.0871793495283746\n",
      "step =  7600 ,  loss_val =  1.3221411925780133\n",
      "step =  8000 ,  loss_val =  1.2903623148385073\n",
      "step =  8400 ,  loss_val =  1.0793863442947973\n",
      "step =  8800 ,  loss_val =  1.382266153776028\n",
      "step =  9200 ,  loss_val =  1.2496224511454823\n",
      "step =  9600 ,  loss_val =  1.345670643886407\n",
      "step =  10000 ,  loss_val =  1.384563378413315\n",
      "step =  10400 ,  loss_val =  1.1893486235124087\n",
      "step =  10800 ,  loss_val =  1.1005393412086601\n",
      "step =  11200 ,  loss_val =  1.230869582874372\n",
      "step =  11600 ,  loss_val =  1.1202853486972941\n",
      "step =  12000 ,  loss_val =  1.2228454379174967\n",
      "step =  12400 ,  loss_val =  1.133286515153607\n",
      "step =  12800 ,  loss_val =  1.1353951098336474\n",
      "step =  13200 ,  loss_val =  1.1541129685592826\n",
      "step =  13600 ,  loss_val =  1.2780429921175407\n",
      "step =  14000 ,  loss_val =  1.1346697199109446\n",
      "step =  14400 ,  loss_val =  1.139494455631904\n",
      "step =  14800 ,  loss_val =  1.2235744861303415\n",
      "step =  15200 ,  loss_val =  1.439131352578339\n",
      "step =  15600 ,  loss_val =  1.253868615633452\n",
      "step =  16000 ,  loss_val =  1.23530783757832\n",
      "step =  16400 ,  loss_val =  1.2396939027209524\n",
      "step =  16800 ,  loss_val =  1.3599302539808138\n",
      "step =  17200 ,  loss_val =  1.177972894612781\n",
      "step =  17600 ,  loss_val =  1.1850285031342067\n",
      "step =  18000 ,  loss_val =  1.125874689999565\n",
      "step =  18400 ,  loss_val =  1.105245358876618\n",
      "step =  18800 ,  loss_val =  1.1782166802897054\n",
      "step =  19200 ,  loss_val =  1.2826280561187737\n",
      "step =  19600 ,  loss_val =  1.172153476294313\n",
      "step =  20000 ,  loss_val =  1.330531028267187\n",
      "step =  20400 ,  loss_val =  1.3263874298492866\n",
      "step =  20800 ,  loss_val =  1.2368586948969602\n",
      "step =  21200 ,  loss_val =  1.195456242884307\n",
      "step =  21600 ,  loss_val =  1.1818968551229188\n",
      "step =  22000 ,  loss_val =  1.1759561001561218\n",
      "step =  22400 ,  loss_val =  1.0906095746050288\n",
      "step =  22800 ,  loss_val =  1.2045595763946961\n",
      "step =  23200 ,  loss_val =  1.1049040223519988\n",
      "step =  23600 ,  loss_val =  1.0783789434293973\n",
      "step =  24000 ,  loss_val =  1.180202654475197\n",
      "step =  24400 ,  loss_val =  1.3503247577428652\n",
      "step =  24800 ,  loss_val =  1.1782901081038777\n",
      "step =  25200 ,  loss_val =  1.2466997135709728\n",
      "step =  25600 ,  loss_val =  1.2275063701320932\n",
      "step =  26000 ,  loss_val =  1.2159897164061622\n",
      "step =  26400 ,  loss_val =  1.1642369129817536\n",
      "step =  26800 ,  loss_val =  1.458882678698725\n",
      "step =  27200 ,  loss_val =  1.0521096986159513\n",
      "step =  27600 ,  loss_val =  1.377469449771774\n",
      "step =  28000 ,  loss_val =  1.1625848904133116\n",
      "step =  28400 ,  loss_val =  1.2547538026636884\n",
      "step =  28800 ,  loss_val =  1.1796477827696699\n",
      "step =  29200 ,  loss_val =  1.3090067447872245\n",
      "step =  29600 ,  loss_val =  1.2871132016697016\n",
      "step =  30000 ,  loss_val =  1.2938373579780027\n",
      "step =  30400 ,  loss_val =  1.1044176716301102\n",
      "step =  30800 ,  loss_val =  1.2293970351260448\n",
      "step =  31200 ,  loss_val =  1.262356629216082\n",
      "step =  31600 ,  loss_val =  1.054174679983812\n",
      "step =  32000 ,  loss_val =  1.173423390194392\n",
      "step =  32400 ,  loss_val =  1.1397429046010843\n",
      "step =  32800 ,  loss_val =  1.210091168198249\n",
      "step =  33200 ,  loss_val =  1.0912585605565222\n",
      "step =  33600 ,  loss_val =  1.2127021542560648\n",
      "step =  34000 ,  loss_val =  1.276449892484129\n",
      "step =  34400 ,  loss_val =  1.2964755427415608\n",
      "step =  34800 ,  loss_val =  5.4687688145209545\n",
      "step =  35200 ,  loss_val =  1.2128769748704984\n",
      "step =  35600 ,  loss_val =  1.1861988370024985\n",
      "step =  36000 ,  loss_val =  1.054662279615944\n",
      "step =  36400 ,  loss_val =  1.2112802934090403\n",
      "step =  36800 ,  loss_val =  1.1267227533540118\n",
      "step =  37200 ,  loss_val =  1.2048043872203076\n",
      "step =  37600 ,  loss_val =  1.1021971157354837\n",
      "step =  38000 ,  loss_val =  1.255698990259384\n",
      "step =  38400 ,  loss_val =  1.1816718320545117\n",
      "step =  38800 ,  loss_val =  1.2116803817976374\n",
      "step =  39200 ,  loss_val =  1.3400684436281907\n",
      "step =  39600 ,  loss_val =  1.0695553582034225\n",
      "step =  40000 ,  loss_val =  1.4021905714600469\n",
      "step =  40400 ,  loss_val =  1.3044487140135863\n",
      "step =  40800 ,  loss_val =  1.1623736361832215\n",
      "step =  41200 ,  loss_val =  1.1844094751621597\n",
      "step =  41600 ,  loss_val =  1.2244770880881444\n",
      "step =  42000 ,  loss_val =  1.267390428779598\n",
      "step =  42400 ,  loss_val =  1.283451480773449\n",
      "step =  42800 ,  loss_val =  1.2425731378209184\n",
      "step =  43200 ,  loss_val =  1.209965575128016\n",
      "step =  43600 ,  loss_val =  1.2143633581705044\n",
      "step =  44000 ,  loss_val =  1.286646200708367\n",
      "step =  44400 ,  loss_val =  1.324862835628479\n",
      "step =  44800 ,  loss_val =  1.1124203175331027\n",
      "step =  45200 ,  loss_val =  1.4904863840204656\n",
      "step =  45600 ,  loss_val =  1.2435106470894246\n",
      "step =  46000 ,  loss_val =  1.212867045464645\n",
      "step =  46400 ,  loss_val =  1.209072322541362\n",
      "step =  46800 ,  loss_val =  1.2671299013168436\n",
      "step =  47200 ,  loss_val =  1.2232797919991465\n",
      "step =  47600 ,  loss_val =  17.156218027602165\n",
      "step =  48000 ,  loss_val =  1.2623323806099433\n",
      "step =  48400 ,  loss_val =  1.3146643144525947\n",
      "step =  48800 ,  loss_val =  1.1125774827285067\n",
      "step =  49200 ,  loss_val =  5.396978723963601\n",
      "step =  49600 ,  loss_val =  1.1564518450453738\n",
      "step =  50000 ,  loss_val =  1.2719178148137484\n",
      "step =  50400 ,  loss_val =  1.098941212380018\n",
      "step =  50800 ,  loss_val =  1.3318708417501826\n",
      "step =  51200 ,  loss_val =  1.0518135470927754\n",
      "step =  51600 ,  loss_val =  6.345571161541507\n",
      "step =  52000 ,  loss_val =  1.1611268427794155\n",
      "step =  52400 ,  loss_val =  1.3304692637626017\n",
      "step =  52800 ,  loss_val =  8.5859357560727\n",
      "step =  53200 ,  loss_val =  1.2805309651003698\n",
      "step =  53600 ,  loss_val =  1.2247512448196585\n",
      "step =  54000 ,  loss_val =  1.337384526854928\n",
      "step =  54400 ,  loss_val =  1.2792354517013313\n",
      "step =  54800 ,  loss_val =  1.3028457064375678\n",
      "step =  55200 ,  loss_val =  1.1494789763382864\n",
      "step =  55600 ,  loss_val =  1.2487554363666793\n",
      "step =  56000 ,  loss_val =  1.1715838968143815\n",
      "step =  56400 ,  loss_val =  1.2824420648166632\n",
      "step =  56800 ,  loss_val =  18.011928237269988\n",
      "step =  57200 ,  loss_val =  1.2067738287080485\n",
      "step =  57600 ,  loss_val =  1.3080496397101584\n",
      "step =  58000 ,  loss_val =  1.2260696913145845\n",
      "step =  58400 ,  loss_val =  1.2027968607277075\n",
      "step =  58800 ,  loss_val =  1.2655401699483342\n",
      "step =  59200 ,  loss_val =  1.2418721946455917\n",
      "step =  59600 ,  loss_val =  1.2287043814499683\n",
      "step =  0 ,  loss_val =  1.3946318146105319\n",
      "step =  400 ,  loss_val =  1.3950214558073883\n",
      "step =  800 ,  loss_val =  1.1999035468873893\n",
      "step =  1200 ,  loss_val =  1.2567256781438392\n",
      "step =  1600 ,  loss_val =  1.2168071148612138\n",
      "step =  2000 ,  loss_val =  1.18005637030404\n",
      "step =  2400 ,  loss_val =  1.272161634001536\n",
      "step =  2800 ,  loss_val =  1.2214435029858461\n",
      "step =  3200 ,  loss_val =  1.2524639802486859\n",
      "step =  3600 ,  loss_val =  1.2716059648022109\n",
      "step =  4000 ,  loss_val =  1.3533163605385075\n",
      "step =  4400 ,  loss_val =  1.3293835415664692\n",
      "step =  4800 ,  loss_val =  1.0830967280322246\n",
      "step =  5200 ,  loss_val =  1.192453632322636\n",
      "step =  5600 ,  loss_val =  1.1617670891082383\n",
      "step =  6000 ,  loss_val =  1.236768485692613\n",
      "step =  6400 ,  loss_val =  1.2970792073772792\n",
      "step =  6800 ,  loss_val =  1.1399228170119486\n",
      "step =  7200 ,  loss_val =  1.1757985007845446\n",
      "step =  7600 ,  loss_val =  1.390374814121424\n",
      "step =  8000 ,  loss_val =  1.375524470177251\n",
      "step =  8400 ,  loss_val =  1.1982685812882583\n",
      "step =  8800 ,  loss_val =  1.3990358889055388\n",
      "step =  9200 ,  loss_val =  1.4130447416248322\n",
      "step =  9600 ,  loss_val =  1.378981796053338\n",
      "step =  10000 ,  loss_val =  1.4080209424862205\n",
      "step =  10400 ,  loss_val =  1.2010811832537351\n",
      "step =  10800 ,  loss_val =  1.124377989677107\n",
      "step =  11200 ,  loss_val =  1.2580522708566262\n",
      "step =  11600 ,  loss_val =  1.2363677963710675\n",
      "step =  12000 ,  loss_val =  1.2454814716215057\n",
      "step =  12400 ,  loss_val =  1.2436279812271749\n",
      "step =  12800 ,  loss_val =  1.1083603472745753\n",
      "step =  13200 ,  loss_val =  1.1798788632793762\n",
      "step =  13600 ,  loss_val =  1.419502426709639\n",
      "step =  14000 ,  loss_val =  1.1906233192332858\n",
      "step =  14400 ,  loss_val =  1.2367360560003986\n",
      "step =  14800 ,  loss_val =  1.2477651158426966\n",
      "step =  15200 ,  loss_val =  1.464710178756703\n",
      "step =  15600 ,  loss_val =  1.3112227244905599\n",
      "step =  16000 ,  loss_val =  1.2671303101360036\n",
      "step =  16400 ,  loss_val =  1.2573408595350188\n",
      "step =  16800 ,  loss_val =  1.36722453090181\n",
      "step =  17200 ,  loss_val =  1.2223867629865042\n",
      "step =  17600 ,  loss_val =  1.1560776891309374\n",
      "step =  18000 ,  loss_val =  1.206154713068788\n",
      "step =  18400 ,  loss_val =  1.0901383729466594\n",
      "step =  18800 ,  loss_val =  1.1819547710077312\n",
      "step =  19200 ,  loss_val =  1.300894216310408\n",
      "step =  19600 ,  loss_val =  1.2038577282689527\n",
      "step =  20000 ,  loss_val =  1.3407069978298363\n",
      "step =  20400 ,  loss_val =  1.3004511782757622\n",
      "step =  20800 ,  loss_val =  1.2914221060620077\n",
      "step =  21200 ,  loss_val =  1.2173597242088672\n",
      "step =  21600 ,  loss_val =  1.1706501970624963\n",
      "step =  22000 ,  loss_val =  1.2733538169263434\n",
      "step =  22400 ,  loss_val =  1.1897711796048172\n",
      "step =  22800 ,  loss_val =  1.2144841504043629\n",
      "step =  23200 ,  loss_val =  1.159834634603102\n",
      "step =  23600 ,  loss_val =  1.1855882257645969\n",
      "step =  24000 ,  loss_val =  1.2495026398882607\n",
      "step =  24400 ,  loss_val =  1.4014745868776521\n",
      "step =  24800 ,  loss_val =  1.1361713355779708\n",
      "step =  25200 ,  loss_val =  1.2368447423651026\n",
      "step =  25600 ,  loss_val =  1.1942744862960646\n",
      "step =  26000 ,  loss_val =  1.2641704066440407\n",
      "step =  26400 ,  loss_val =  1.1993503133996188\n",
      "step =  26800 ,  loss_val =  1.46212354250595\n",
      "step =  27200 ,  loss_val =  1.1212629413086288\n",
      "step =  27600 ,  loss_val =  1.3954765629599826\n",
      "step =  28000 ,  loss_val =  1.1959070965012222\n",
      "step =  28400 ,  loss_val =  1.3265644007234494\n",
      "step =  28800 ,  loss_val =  1.1671732439528073\n",
      "step =  29200 ,  loss_val =  1.2482696560424273\n",
      "step =  29600 ,  loss_val =  1.2280945474835174\n",
      "step =  30000 ,  loss_val =  1.400668981143157\n",
      "step =  30400 ,  loss_val =  1.1397974398919053\n",
      "step =  30800 ,  loss_val =  1.2716532960884794\n",
      "step =  31200 ,  loss_val =  1.234306245191305\n",
      "step =  31600 ,  loss_val =  1.1232808690103966\n",
      "step =  32000 ,  loss_val =  1.2667784755920142\n",
      "step =  32400 ,  loss_val =  1.1981324864488474\n",
      "step =  32800 ,  loss_val =  1.2924461292789897\n",
      "step =  33200 ,  loss_val =  1.1994078853897008\n",
      "step =  33600 ,  loss_val =  1.2076031658198827\n",
      "step =  34000 ,  loss_val =  1.2597285551291593\n",
      "step =  34400 ,  loss_val =  1.371980700042591\n",
      "step =  34800 ,  loss_val =  1.0923474499499457\n",
      "step =  35200 ,  loss_val =  1.2234183954452096\n",
      "step =  35600 ,  loss_val =  1.2465156518123173\n",
      "step =  36000 ,  loss_val =  1.1327059190780433\n",
      "step =  36400 ,  loss_val =  1.2687099797032355\n",
      "step =  36800 ,  loss_val =  1.0999454328365095\n",
      "step =  37200 ,  loss_val =  1.2037680003765157\n",
      "step =  37600 ,  loss_val =  1.1645415624006992\n",
      "step =  38000 ,  loss_val =  1.2302489860192374\n",
      "step =  38400 ,  loss_val =  1.1825746398103985\n",
      "step =  38800 ,  loss_val =  1.281081843899651\n",
      "step =  39200 ,  loss_val =  1.3388179793096573\n",
      "step =  39600 ,  loss_val =  1.1735702612290606\n",
      "step =  40000 ,  loss_val =  1.489106817347952\n",
      "step =  40400 ,  loss_val =  1.3135616244666268\n",
      "step =  40800 ,  loss_val =  1.2392833155342942\n",
      "step =  41200 ,  loss_val =  1.2061907285384799\n",
      "step =  41600 ,  loss_val =  1.2247163975292044\n",
      "step =  42000 ,  loss_val =  1.3513282854823483\n",
      "step =  42400 ,  loss_val =  1.2490891445239658\n",
      "step =  42800 ,  loss_val =  1.2963049119700423\n",
      "step =  43200 ,  loss_val =  1.2870433608624825\n",
      "step =  43600 ,  loss_val =  1.2302404799160047\n",
      "step =  44000 ,  loss_val =  1.3651531285628686\n",
      "step =  44400 ,  loss_val =  1.2650180310280126\n",
      "step =  44800 ,  loss_val =  1.2398338435211071\n",
      "step =  45200 ,  loss_val =  1.4588133301192334\n",
      "step =  45600 ,  loss_val =  1.3061141182528708\n",
      "step =  46000 ,  loss_val =  1.2617403872219854\n",
      "step =  46400 ,  loss_val =  1.2701671123477518\n",
      "step =  46800 ,  loss_val =  1.2333723265963648\n",
      "step =  47200 ,  loss_val =  1.2388598068447745\n",
      "step =  47600 ,  loss_val =  14.170351472095295\n",
      "step =  48000 ,  loss_val =  1.358968940279914\n",
      "step =  48400 ,  loss_val =  1.361004696176758\n",
      "step =  48800 ,  loss_val =  1.1269901309284651\n",
      "step =  49200 ,  loss_val =  6.627410204738347\n",
      "step =  49600 ,  loss_val =  1.1021973777387615\n",
      "step =  50000 ,  loss_val =  1.3535405496663782\n",
      "step =  50400 ,  loss_val =  1.1837742892812897\n",
      "step =  50800 ,  loss_val =  1.4355296566755822\n",
      "step =  51200 ,  loss_val =  1.0827304359179872\n",
      "step =  51600 ,  loss_val =  6.647945551214936\n",
      "step =  52000 ,  loss_val =  1.1666079763262176\n",
      "step =  52400 ,  loss_val =  1.3101102419095827\n",
      "step =  52800 ,  loss_val =  13.078022439821696\n",
      "step =  53200 ,  loss_val =  1.3004074324121826\n",
      "step =  53600 ,  loss_val =  1.172906768221763\n",
      "step =  54000 ,  loss_val =  1.3892007138825506\n",
      "step =  54400 ,  loss_val =  1.2844918128258584\n",
      "step =  54800 ,  loss_val =  1.2969526878983775\n",
      "step =  55200 ,  loss_val =  1.202541735809496\n",
      "step =  55600 ,  loss_val =  1.236247040778935\n",
      "step =  56000 ,  loss_val =  1.2765548884780613\n",
      "step =  56400 ,  loss_val =  1.3232262062290199\n",
      "step =  56800 ,  loss_val =  14.508564127609247\n",
      "step =  57200 ,  loss_val =  1.256221933466467\n",
      "step =  57600 ,  loss_val =  1.4040900397047629\n",
      "step =  58000 ,  loss_val =  1.2097642715396795\n",
      "step =  58400 ,  loss_val =  1.2521186859901756\n",
      "step =  58800 ,  loss_val =  1.2749296173847884\n",
      "step =  59200 ,  loss_val =  1.264705449006163\n",
      "step =  59600 ,  loss_val =  1.2265842379791656\n",
      "\n",
      "Elapsed time =  0:00:52.986296\n",
      "Current Accuracy =  96.46000000000001  %\n"
     ]
    }
   ],
   "source": [
    "input_nodes = 784\r\n",
    "hidden_nodes = 100\r\n",
    "output_nodes = 10\r\n",
    "learning_rate = 0.3\r\n",
    "epochs = 5\r\n",
    "\r\n",
    "nn = NeuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\r\n",
    "\r\n",
    "start_time = datetime.now()\r\n",
    "\r\n",
    "for i in range(epochs):\r\n",
    "    for step in range(len(train_data)):\r\n",
    "\r\n",
    "        # input_data, target_data normalize\r\n",
    "        target_data = np.zeros(output_nodes) + 0.01\r\n",
    "        target_data[int(train_data[step, 0])] = 0.99\r\n",
    "\r\n",
    "        input_data = ((train_data[step, 1:] / 255.0)*0.99) + 0.01\r\n",
    "\r\n",
    "        nn.train(np.array(input_data, ndmin=2), np.array(target_data, ndmin=2))\r\n",
    "\r\n",
    "        if step % 400 == 0:\r\n",
    "            print(\"step = \", step, \",  loss_val = \", nn.loss_val())\r\n",
    "\r\n",
    "end_time = datetime.now()\r\n",
    "\r\n",
    "# 학습을 위해 수행한 총 시간 출력\r\n",
    "print(\"\\nElapsed time = \", end_time-start_time)\r\n",
    "\r\n",
    "# 정확도 출력\r\n",
    "(_matched_list, _not_matched_list) = nn.accuracy(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오차 역전파 공식\r\n",
    "## [1] 출력층의 출력값과 정답(Target)을 이용하여, 출력층 손실 계산\r\n",
    "- 출력층 손실 = (출력층 출력 - 정답) * 출력층 출력(1 - 출력층 출력)\r\n",
    "\r\n",
    "## [2] 은닉층에서의 (가상의) 손실 loss_3, loss_2 등을 계산할 경우, 현재층(current layer), 이전층(previous layer), 다음층(next layer) 개념을 도입하여 동일한 패턴으로 반복 계산\r\n",
    "- 은닉층의 현재층  손실 = (다음층 손실 · 다음층에 적용되는 가중치 W^T) * 현재층 출력(1 - 현재층 출력)\r\n",
    "\r\n",
    "## [3] 계산된 각 층의 출력값과 손실값을 이용하여,\r\n",
    "- 현재층의 바이어스 변화율 ∂E/∂b = 현재층 손실\r\n",
    "- 현재층에 적용되는 가중치 변화율 ∂E/∂W = (이전층 출력)^T · 현재층 손실"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "247ab06e135bb35fa78c5eff31b2a9a0050dcb5fb773c2631d2a29ac689eeccb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}